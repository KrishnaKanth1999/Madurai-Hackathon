{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "3d9e6727-dbbb-82b3-7807-a6b25a7519b0",
        "_uuid": "0e35811358ae154e840dbe3fae2139f5aab73ed5"
      },
      "cell_type": "markdown",
      "source": "\nHere's an attempt to create a recommendation engine with this dataset. Our Naive assumption is that a person's taste in film does not evolve with time."
    },
    {
      "metadata": {
        "_cell_guid": "26da2a2f-dd97-c4cd-6bd8-a7570b95dce9",
        "_uuid": "ca72f496ab8078a1eb4397ff2523752fb8818143",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n%pylab inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9a073912-e84f-f2aa-51f1-853a89b8b8e6",
        "_uuid": "cf1db35511850ea57fe71a7203bf721b2d4e0a69",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "credits = load_tmdb_credits(\"../input/tmdb_5000_credits.csv\")\nmovies = load_tmdb_movies(\"../input/tmdb_5000_movies.csv\")\nprint('o')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5ec12d92-e36c-f201-50f3-15b3f9333fa0",
        "_uuid": "e73e49ae6afa6efbee51979469947b42c1effb73"
      },
      "cell_type": "markdown",
      "source": "Since cleaning the data is not the focus of this notebook, I'll just dump it all in one cell. That way we can skip over to the nice parts."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19011a4ce40cb56935c317a553bd38e63e3b0f4c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import json\nimport pandas as pd\n#___________________________\ndef load_tmdb_movies(path):\n    df = pd.read_csv(path)\n    df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.date())\n    json_columns = ['genres', 'keywords', 'production_countries',\n                    'production_companies', 'spoken_languages']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n#___________________________\ndef load_tmdb_credits(path):\n    df = pd.read_csv(path)\n    json_columns = ['cast', 'crew']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n#___________________\nLOST_COLUMNS = [\n    'actor_1_facebook_likes',\n    'actor_2_facebook_likes',\n    'actor_3_facebook_likes',\n    'aspect_ratio',\n    'cast_total_facebook_likes',\n    'color',\n    'content_rating',\n    'director_facebook_likes',\n    'facenumber_in_poster',\n    'movie_facebook_likes',\n    'movie_imdb_link',\n    'num_critic_for_reviews',\n    'num_user_for_reviews']\n#____________________________________\nTMDB_TO_IMDB_SIMPLE_EQUIVALENCIES = {\n    'budget': 'budget',\n    'genres': 'genres',\n    'revenue': 'gross',\n    'title': 'movie_title',\n    'runtime': 'duration',\n    'original_language': 'language',\n    'keywords': 'plot_keywords',\n    'vote_count': 'num_voted_users'}\n#_____________________________________________________\nIMDB_COLUMNS_TO_REMAP = {'imdb_score': 'vote_average'}\n#_____________________________________________________\ndef safe_access(container, index_values):\n    # return missing value rather than an error upon indexing/key failure\n    result = container\n    try:\n        for idx in index_values:\n            result = result[idx]\n        return result\n    except IndexError or KeyError:\n        return pd.np.nan\n#_____________________________________________________\ndef get_director(crew_data):\n    directors = [x['name'] for x in crew_data if x['job'] == 'Director']\n    return safe_access(directors, [0])\n#_____________________________________________________\ndef pipe_flatten_names(keywords):\n    return '|'.join([x['name'] for x in keywords])\n#_____________________________________________________\ndef convert_to_original_format(movies, credits):\n    tmdb_movies = movies.copy()\n    tmdb_movies.rename(columns=TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES, inplace=True)\n    tmdb_movies['title_year'] = pd.to_datetime(tmdb_movies['release_date']).apply(lambda x: x.year)\n    # I'm assuming that the first production country is equivalent, but have not been able to validate this\n    tmdb_movies['country'] = tmdb_movies['production_countries'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['language'] = tmdb_movies['spoken_languages'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['director_name'] = credits['crew'].apply(get_director)\n    tmdb_movies['actor_1_name'] = credits['cast'].apply(lambda x: safe_access(x, [1, 'name']))\n    tmdb_movies['actor_2_name'] = credits['cast'].apply(lambda x: safe_access(x, [2, 'name']))\n    tmdb_movies['actor_3_name'] = credits['cast'].apply(lambda x: safe_access(x, [3, 'name']))\n    tmdb_movies['genres'] = tmdb_movies['genres'].apply(pipe_flatten_names)\n    tmdb_movies['plot_keywords'] = tmdb_movies['plot_keywords'].apply(pipe_flatten_names)\n    return tmdb_movies\nprint('Step 1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53e10a7abf7ef9dc235be53427e0f3529afd84ab",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math, nltk, warnings\nfrom nltk.corpus import wordnet\nfrom sklearn import linear_model\nfrom sklearn.neighbors import NearestNeighbors\nfrom fuzzywuzzy import fuzz\nfrom wordcloud import WordCloud, STOPWORDS\nplt.rcParams[\"patch.force_edgecolor\"] = True\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"last_expr\"\npd.options.display.max_columns = 50\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nPS = nltk.stem.PorterStemmer()\n#__________________\n# load the dataset\n\ndf = convert_to_original_format(movies, credits)\nprint('Shape:',df.shape)\n#__________________________________________\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a01e107b872a6168d684df2fbfae65134b52a4ea",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "60f5d03e7445edffa5200589d71c5861305214b1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df=df.drop(['homepage','tagline','status','spoken_languages','release_date','production_companies','production_countries','original_title','overview','vote_average'],axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6dacea168a6d50fd382723a01b577ba8d5c6700",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df.info(verbose=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "71196701-30ef-1c69-2736-7aa8ae69aac0",
        "_uuid": "8fdf0c6eb6468d5a54f184ec175171769be8891b",
        "trusted": true,
        "_kg_hide-input": false,
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "first_actors = set(df.actor_1_name.unique())\nsecond_actors = set(df.actor_2_name.unique())\nthird_actors = set(df.actor_3_name.unique())\nprint('Those only in first name', len(first_actors - second_actors - third_actors))\nprint('Those only in second name', len(second_actors - first_actors - third_actors))\nprint('Those only in third name', len(third_actors - first_actors - second_actors))\nunique_genre_labels = set()\nfor genre_flags in df.genres.str.split('|').values:\n    unique_genre_labels = unique_genre_labels.union(set(genre_flags))\nfor label in unique_genre_labels:\n    df['Genre='+label] = df.genres.str.contains(label).astype(int)\ndf = df.drop('genres', axis=1)\n\n# Titles are supposed to be unique right?\nif len(df.drop_duplicates(subset=['movie_title',\n                                  'title_year'])) < len(df):\n    print('Duplicate Titles Exist')\n    # Let's see these duplicates.\n    duplicates = df[df.movie_title.map(df.movie_title.value_counts() > 1)]\n    duplicates.sort('movie_title')[['movie_title', 'title_year']]\n    # Looks like there are duplicates after all. Let's drop those.\n    df = df.drop_duplicates(subset=['movie_title', 'title_year'])\n\n    duplicates = df[df.movie_title.map(df.movie_title.value_counts() > 1)]\n    duplicates.sort('movie_title')[['movie_title', 'title_year']]\n    # Looks like there are duplicates after all. Let's drop those.\n    df = df.drop_duplicates(subset=['movie_title', 'title_year'])\n    # df.info()\ncounts = df.language.value_counts()\ndf.language = df.language.map(counts)\n#df.language\ncount = df.country.value_counts()\ndf.country = df.country.map(count)\n\nprint('1')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af7fe96d219c4aff4194e358d8bcbbca5c64715a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('start')\nunique_words = set()\nfor wordlist in df.plot_keywords.str.split('|').values:\n    if wordlist is not np.nan:\n        unique_words = unique_words.union(set(wordlist))\nplot_wordbag = list(unique_words)\nfor word in plot_wordbag:\n    df['plot_has_' + word.replace(' ', '-')] = df.plot_keywords.str.contains(word).astype(float)\ndf = df.drop('plot_keywords', axis=1)\n# Is anything left to be done other than imputing?\nprint(df.select_dtypes(include=['O']).columns)\n# We replace director name with counts of movies they've done\ndf.director_name = df.director_name.map(df.director_name.value_counts())\n# We replace actor names with the number of movies they appear in.\ncounts = pd.concat([df.actor_1_name, df.actor_2_name, df.actor_3_name]).value_counts()\n#counts.head()\ndf.actor_1_name = df.actor_1_name.map(counts)\ndf.actor_2_name = df.actor_2_name.map(counts)\ndf.actor_3_name = df.actor_3_name.map(counts)\n# I have no clue what to do with the title. I'll keep it for now in order to search by name\n\n# Let's check if anything is left as object\ndf.select_dtypes(include=['O']).columns\n# Titles are supposed to be unique right?\nprint('2')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b52b8647-e1e6-f60d-ac28-8f61a7d29a36",
        "_uuid": "b26b87cbc5a4b7f5efb4deac1b0a732ce73bdf2b",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(df['director_name'].isnull().sum())\nprint(df['popularity'].isnull().sum())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e884a5d2-f853-4654-eea4-1aaa3f0c2f9b",
        "_uuid": "0d2458781d789eb1318ce389ac61f5fdf1f0ec8e"
      },
      "cell_type": "markdown",
      "source": "# Now the data is clean enough. Recommend already!\nIt's filled with holes though. Pun intended. :D\n\nI wanted to try out some fancy imputation (there's a package by that name too) so here goes."
    },
    {
      "metadata": {
        "_cell_guid": "40fe4659-32ce-b5e0-7a44-a6b8206ed607",
        "_uuid": "d192887b76e718927c347b0feda7b9d41cf3892b",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# hold your horses, we still need to fill those missing values.\nnew_style = {'grid': False}\nmatplotlib.rc('axes', **new_style)\nplt.matshow(~df.isnull())\nplt.title('Missing values in the data')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2a879086-6506-4b81-acec-9c090ff7f886",
        "_uuid": "add3fad1e9a1f6ea3d4bc32d84682f359f8c493a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Let's get those rows which are mostly incomplete. I suspect this was because of our\n# new features being created from old ones which were null.\nnullcount = df.isnull().sum(axis=1)\n# Let's just keep those who have less than a hundred missing values\nndf = df.dropna(thresh=100)\nprint(ndf.shape, df.shape)\n# Let's see those nulls again\n\nplt.matshow(~ndf.isnull())\nplt.title('Missing values in the data')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bea5151d-285f-e2f9-f609-b3c17ed8bc70",
        "_uuid": "be48c8cb59e6cb521149ee735bd6072a07e9a067",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e0b588d0a357c8914eebb72fd29b0f72984c29ce",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# We'll treat fillna as a regression / classification problem here.\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\ndef reg_class_fill(df, column, classifier):\n    \"\"\"Treat missing values as a classification / regresion problem\"\"\"\n    ndf = df.dropna(subset=[col for col in df.columns if col != column])\n    nullmask = ndf[column].isnull()\n    \n    train, test  = ndf[~nullmask], ndf[nullmask]\n    \n    train_x, train_y = train.drop(column, axis=1), train[column]\n    classifier.fit(train_x, train_y)\n    if len(test) > 0:\n        test_x, test_y = test.drop(column, axis=1), test[column]\n        values = classifier.predict(test_x)\n        test_y = values\n        new_x, new_y = pd.concat([train_x, test_x]), pd.concat([train_y, test_y])\n        newdf = new_x[column] = new_y\n        return newdf\n    else:\n        return ndf\nprint('step 4')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b3bcdbcf-d9b0-a2a4-eba0-874afd2809c5",
        "_uuid": "94c28c12a3e0b1bd1373d774b5249c5a043e126e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "r, c = KNeighborsRegressor, KNeighborsClassifier  # Regress or classify\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(ndf.movie_title)\nndf.movie_title = title_encoder.transform(ndf.movie_title)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "10652ebc-f97b-f755-9c1d-9d02502b5e03",
        "_uuid": "e147ae7aa237a4f28a01d7dd2fb4f95abaebb3e9",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\nprint(ndf.popularity)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5d914191-bc45-552a-06d8-428beab17cba",
        "_uuid": "eab549354a3388c5f04751134e909da0a0dd39e5",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Since our imputation will impact other imputations, we specify an order\n# Typically we should do this independently and then combine the results, but meh for now\nimpute_order = [('budget', r)]\nfor col, classifier in impute_order:\n    ndf = reg_class_fill(ndf, col, classifier())\n    print(col, 'Done')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9b2f424a-bf29-3947-9da7-54b5f0ea63fb",
        "_uuid": "8220af74831f8cb66866ee3f0569fe7072c150da",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Again we check for what else needs to be imputed.\nndf[ndf.columns[:25]].isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "840b2ac6-3c18-86dd-fbe5-1e585d3469f2",
        "_uuid": "d6290674b931649d8ab739286c2903e3f7bea2a0",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Did we get everything?\nndf.isnull().sum().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "231e40ad-7f59-b553-02c7-6b3b987550fd",
        "_uuid": "a6341eef7dec92c8d734072c88551966c7e34e78",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# YAY! We did indeed get everything, though it may not have been very good.\n# Now we redo the movie title transformation for our searches.\ntitles = title_encoder.inverse_transform(ndf.movie_title)\n\n#titles = [i.lower().strip() for i in titles]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "04b7b550-410b-75c2-0386-e71968cf3a51",
        "_uuid": "abb49cea8f64fd170c68a6bba48c7063f9c66ac1"
      },
      "cell_type": "markdown",
      "source": "# And we are ready to recommend stuff to you love :D\nWe build a simple KD tree recommender."
    },
    {
      "metadata": {
        "_cell_guid": "430d50a5-e19d-464a-d147-0f63f52628a4",
        "_uuid": "130b84fef762bf3ef39ff3b28d6f73692a3bb700",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Give us 5 movies that you liked\ndef get_movies(names):\n    movies = []\n    for name in names:\n        found = [i for i in titles if name.lower() in i.lower()]\n        \n        if len(found) > 0:\n            movies.append(found[0])\n            print(name, ': ', found, 'added', movies[-1], 'to movies')\n        else:\n            print(name, ': ', found)\n    print('-'*10)\n    moviecodes = title_encoder.transform(movies)\n    return moviecodes, movies\nnames = ['fight club', 'gump', # This one is Forrest Gump\n                 'usual suspects', 'silence of the lambs']\nmoviecodes, movies = get_movies(names)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1ad0d1dc-6463-34e2-c607-1621db382254",
        "_uuid": "47b60fe571f12153a6ba46d576a7348c97065d57",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data = ndf.drop('movie_title', axis=1)\ndata = MinMaxScaler().fit_transform(data)\nprint(data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e5232c5b-a853-d9d3-e347-ccf00b371052",
        "_uuid": "3431f55dbd2d4b027a308ba5bdaa302a15f9eb26",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# We assume KNN's assumptions as valid and proceede to compute a distance_matrix\nfrom sklearn.neighbors import KDTree\nfrom collections import Counter",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5525c269-3a8a-bbc4-30fe-80cda7104fab",
        "_uuid": "5992a52ad781327f3e3c0c89be7cbddcb527db05",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "movies",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "cb779843-2087-0693-0037-338f7cf57714",
        "_uuid": "3175f9d831649b16f84e983af29f5626c98b76f4",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "titles",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79828a7f-f51a-8596-8a1e-604c35c73ce3",
        "_uuid": "096db83325f98088b93a3e28127298984c934244",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tree = KDTree(data, leaf_size=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "052892fb-85b8-e806-f8ee-575cc30b8eb1",
        "_uuid": "fe4eb5e4952f8fa306e2850b47ba371f01e521b9",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def recommend(movies, tree, titles, data):\n    \"\"\"\n    It is assumed that the movies are in order of decreasing like-able-ness\n    Recommend movies on the basis of the KDTree generated.\n    Return them in order of increasing distance form knowns.\n    \"\"\"\n    titles = list(titles)\n    length, recommendations = len(movies) + 1,[]\n    \n    for i, movie in enumerate(movies):\n        weight = length - i\n        dist, index = tree.query([data[titles.index(movie)]], k=3)\n        for d, m in zip(dist[0], index[0]):\n            recommendations.append((d*weight, titles[m]))\n    recommendations.sort()\n    # Stuff is reorganized by frequency.\n    \n\n    rec = [i[1].strip() for i in recommendations if i[1] not in movies]\n    print(rec,'kir')\n    rec = [i[1] for i in sorted([(v, k) for k, v in Counter(rec).items()],\n                                reverse=True)]\n    return rec",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7c63a473-5399-e8f3-8d23-cd24acfdcf12",
        "_uuid": "8ab81747218c52e571f58c79d46404124d1d9c1e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\nrec = recommend(movies, tree, titles, data)\n\nprint('Rank | Movie')\nprint('-----|------')\nfmt = '{}.   | {}'\nfor index, movie in enumerate(rec[:10]):\n    print(fmt.format(index + 1, movie))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "da79ebde-e74e-7a8f-4b76-0df5bab6d8b6",
        "_uuid": "ab0707baf21a9ad190c62a28bc5b73bc96f375f2"
      },
      "cell_type": "markdown",
      "source": "# Tadaa!\nIt's not very neat and awesome! But I did like Untraceable to be honest. \nSome movies are recommended twice! Probably because they are quiet close to multiple choices.\n\n## What else can be done?\n\n- Feature generation: I've done a nasty job of generating features. That could be cleaned up.\n- Imputation: A better way of imputing is welcome. Perhaps even need I say.\n- Some other recommendation method: So far I've only been able to discover KDTrees. If someone could write another one, awesome!\n\n*Upvote* to show your appreciation. :D\n\n# The final product\n\n1. Get movie titles\n2. Recommend"
    },
    {
      "metadata": {
        "_cell_guid": "9fe7a2e8-4792-a6e6-2a6a-6e71fd32691d",
        "_uuid": "12c2209dc2ca407b0472c52cb49c12b72c3c1839",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "names = ['unbreakable','django unchained','the sin city','the hobbit','everest','unknown','the grey','superman'] # dedicated to A.S.\nmoviecodes, movies = get_movies(names)\nrec = recommend(movies, tree, titles, data)\nprint('-'*50)\nprint('Recommending on the basis of the above movies')\nprint('-'*50)\nprint()\nprint('+-----|------')\nprint('|Rank | Movie')\nprint('+-----|------')\nfmt = '|{}.   | {}'\nfor index, movie in enumerate(rec[:len(rec)]):\n    print(fmt.format(index + 1, movie))\nprint('+-----|------')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ef5f09ed8c3ef87ea9fc35a09338032b0701261",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}